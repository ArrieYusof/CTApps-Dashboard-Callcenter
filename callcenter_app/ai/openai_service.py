# Version: 0.1
# Last Modified: 2025-08-24
# Changes: Initial OpenAI integration for AI-powered call center insights
"""
OpenAI Integration Service for Call Center AI Analytics
Handles prompt engineering and API communication for contextual insights
"""
from openai import OpenAI
import json
import os
from typing import Dict, Any, Optional
from datetime import datetime
import asyncio

class OpenAIInsightGenerator:
    """OpenAI integration for generating contextual KPI insights"""
    
    def __init__(self, api_key: Optional[str] = None):
        """Initialize OpenAI client with API key"""
        api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4"  # Use GPT-4 for better analysis
        self.max_tokens = 800
        self.temperature = 0.3  # Lower for more consistent, factual responses
    
    async def generate_kpi_insights(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate AI insights based on RAG context"""
        try:
            # Build contextual prompt
            prompt = self._build_insight_prompt(context)
            
            # Call OpenAI API (sync for now, can be made async later)
            response = self._call_openai_sync(prompt)
            
            # Parse and structure response
            insights = self._parse_insights_response(response, context['kpi_type'])
            
            return {
                'success': True,
                'insights': insights,
                'generated_at': datetime.now().isoformat(),
                'model_used': self.model
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'fallback_insights': self._get_fallback_insights(context['kpi_type'])
            }
    
    def _build_insight_prompt(self, context: Dict[str, Any]) -> str:
        """Build contextual prompt for OpenAI based on RAG data - Markdown Output"""
        kpi_type = context['kpi_type']
        current_value = context['current_value']
        
        # Get comparative metrics for context
        comp_metrics = context.get('comparative_metrics', {})
        patterns = context.get('relevant_patterns', [])
        best_practices = context.get('best_practices', [])
        
        base_prompt = f"""
You are an expert call center analytics AI assistant. Analyze the {kpi_type} data and provide actionable insights in clean markdown format.

CURRENT METRICS:
- KPI: {kpi_type}
- Current Value: {current_value}
- Timestamp: {context.get('timestamp', 'N/A')}

COMPARATIVE METRICS:
{json.dumps(comp_metrics, indent=2) if comp_metrics else 'No comparative data available'}

KNOWN PATTERNS:
{chr(10).join(f"• {pattern}" for pattern in patterns) if patterns else 'No patterns available'}

BEST PRACTICES:
{chr(10).join(f"• {practice}" for practice in best_practices) if best_practices else 'Standard practices apply'}

Please provide a comprehensive analysis using this EXACT markdown structure:

# 📊 {kpi_type.replace('_', ' ').title()} Analysis - {current_value}

## 🔍 Current Status
[Brief assessment of current performance with key insights and context]

## 📈 Trend Analysis  
[Analysis of patterns and trends with specific data points]
- **Recent trend**: [direction and magnitude]
- **Comparison**: [vs averages/targets]
- **Pattern observed**: [key insight]

## 💡 Recommendations
1. **Immediate Action**: [What to do right now]
2. **Short-term (2-4 hours)**: [Near-term actions]
3. **Monitoring**: [What to watch closely]

## 🔮 Predictions (Next 2-4 Hours)
[Specific forecasts with confidence indicators using ⚠️ 🟡 🟢 for risk levels]

## ⚠️ Risk Assessment
**RISK LEVEL**: [LOW/MODERATE/HIGH] - [Brief explanation of main risks and mitigation]

---
*Analysis generated by AI using enhanced context data*

Use emojis, **bold text**, bullet points, and clear formatting. Be specific with numbers and timeframes. Keep it concise but actionable.
"""
        
        return base_prompt
    
    def generate_markdown_insights(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate AI insights in markdown format - Enhanced with detailed logging"""
        
        print(f"\n🤖 AI INFO: ===== OPENAI REQUEST DETAILS =====")
        print(f"🤖 AI INFO: Context received:")
        print(f"🤖 AI INFO: - KPI Type: {context.get('kpi_type', 'Unknown')}")
        print(f"🤖 AI INFO: - Current Value: {context.get('current_value', 'N/A')}")
        print(f"🤖 AI INFO: - Context Data Keys: {list(context.keys())}")
        
        # Log RAG context details
        if 'comparative_metrics' in context:
            print(f"🤖 AI INFO: - Comparative Metrics: {context['comparative_metrics']}")
        if 'historical_trends' in context:
            print(f"🤖 AI INFO: - Historical Trends: {context['historical_trends']}")
        if 'relevant_patterns' in context:
            print(f"🤖 AI INFO: - Relevant Patterns: {context['relevant_patterns']}")
        
        try:
            # Build the complete prompt
            prompt = self._build_insight_prompt(context)
            
            print(f"\n🤖 AI INFO: ===== FULL OPENAI PROMPT =====")
            print(f"🤖 AI INFO: Prompt Length: {len(prompt)} characters")
            print(f"🤖 AI INFO: Full Prompt:")
            print(f"🤖 AI INFO: {'-'*60}")
            print(prompt)
            print(f"🤖 AI INFO: {'-'*60}")
            print(f"🤖 AI INFO: ===== END OF PROMPT =====\n")
            
            # Make API call
            print(f"🤖 AI INFO: Making OpenAI API call...")
            response = self._call_openai_sync(prompt)
            
            if response and response.strip():
                print(f"\n🤖 AI INFO: ===== OPENAI RESPONSE =====")
                print(f"🤖 AI INFO: Response Length: {len(response)} characters")
                print(f"🤖 AI INFO: Full Response:")
                print(f"🤖 AI INFO: {'-'*60}")
                print(response)
                print(f"🤖 AI INFO: {'-'*60}")
                print(f"🤖 AI INFO: ===== END OF RESPONSE =====\n")
                
                return {
                    'success': True,
                    'markdown_content': response,
                    'source': 'openai_api',
                    'model': self.model,
                    'timestamp': self._get_current_timestamp()
                }
            else:
                print(f"🤖 AI INFO: ❌ Empty response from OpenAI")
                fallback_content = self._get_markdown_fallback(context.get('kpi_type', 'unknown'), context.get('current_value', 0))
                return {
                    'success': False,
                    'markdown_content': fallback_content,
                    'source': 'fallback',
                    'timestamp': self._get_current_timestamp()
                }
                
        except Exception as e:
            print(f"🤖 AI INFO: ❌ OpenAI API error: {str(e)}")
            fallback_content = self._get_markdown_fallback(context.get('kpi_type', 'unknown'), context.get('current_value', 0), str(e))
            return {
                'success': False,
                'markdown_content': fallback_content,
                'source': 'error_fallback',
                'timestamp': self._get_current_timestamp(),
                'error': str(e)
            }

    def _get_current_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _get_markdown_fallback(self, kpi_type: str, current_value: Any, error: str = "") -> str:
        """Generate markdown fallback when OpenAI is unavailable"""
        kpi_display = kpi_type.replace('_', ' ').title()
        
        fallback_content = f"""# 📊 {kpi_display} Analysis - {current_value}

## 🔍 Current Status
AI analysis service is temporarily unavailable. Current {kpi_display.lower()} value is **{current_value}**.

## 📈 Trend Analysis  
- **Data Source**: Historical patterns and operational knowledge
- **Status**: Manual analysis recommended
- **Context**: Using fallback insights based on best practices

## 💡 Recommendations
1. **Immediate Action**: Monitor current performance closely
2. **Short-term**: Review historical trends manually  
3. **Monitoring**: Use standard operational procedures

## 🔮 Predictions (Next 2-4 Hours)
🟡 **MODERATE CONFIDENCE** - Use historical averages and operational experience for forecasting

## ⚠️ Risk Assessment
**RISK LEVEL**: MODERATE - Enhanced manual monitoring recommended during AI service outage

---
*Fallback analysis - AI service temporarily unavailable*
{f"*Error: {error}*" if error else ""}
"""
        
        return fallback_content
    
    def _call_openai_sync(self, prompt: str) -> str:
        """Synchronous call to OpenAI API using new v1.x format"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert call center analytics AI that provides actionable insights based on data analysis. Always be specific, concise, and focus on actionable recommendations."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                top_p=0.9
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            raise Exception(f"OpenAI API call failed: {str(e)}")
    
    def _parse_insights_response(self, response: str, kpi_type: str) -> Dict[str, Any]:
        """Parse OpenAI response into structured insights"""
        sections = {
            'status': '',
            'trends': '',
            'recommendations': [],
            'predictions': '',
            'risks': '',
            'raw_response': response
        }
        
        # Simple parsing based on headers (could be improved with regex)
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if 'CURRENT STATUS' in line.upper():
                current_section = 'status'
            elif 'TREND ANALYSIS' in line.upper():
                current_section = 'trends'
            elif 'RECOMMENDATIONS' in line.upper() or 'ACTIONABLE' in line.upper():
                current_section = 'recommendations'
            elif 'PREDICTIONS' in line.upper():
                current_section = 'predictions'
            elif 'RISK' in line.upper():
                current_section = 'risks'
            elif current_section and line.startswith('•') or line.startswith('-'):
                if current_section == 'recommendations':
                    sections['recommendations'].append(line.lstrip('•- '))
                else:
                    sections[current_section] += line + '\n'
            elif current_section and line:
                sections[current_section] += line + ' '
        
        return sections
    
    def _get_fallback_insights(self, kpi_type: str) -> Dict[str, Any]:
        """Provide fallback insights when OpenAI is unavailable"""
        fallbacks = {
            'call_volume': {
                'status': 'Current call volume analysis unavailable',
                'recommendations': [
                    'Monitor queue levels closely',
                    'Consider adjusting staffing for peak hours',
                    'Review historical patterns for capacity planning'
                ],
                'risks': 'Unable to generate AI predictions. Use historical averages.'
            },
            'customer_satisfaction': {
                'status': 'CSAT analysis service temporarily unavailable',
                'recommendations': [
                    'Review recent customer feedback',
                    'Focus on first-call resolution improvement',
                    'Monitor response times closely'
                ],
                'risks': 'Manual review of satisfaction trends recommended.'
            },
            'sla_monitoring': {
                'status': 'SLA analysis service unavailable',
                'recommendations': [
                    'Monitor current performance against targets',
                    'Prepare contingency staffing plans',
                    'Review breach prevention protocols'
                ],
                'risks': 'Enhanced manual monitoring recommended during peak hours.'
            }
        }
        
        return fallbacks.get(kpi_type, {
            'status': 'AI insights temporarily unavailable',
            'recommendations': ['Manual analysis recommended'],
            'risks': 'Use standard operational procedures'
        })
